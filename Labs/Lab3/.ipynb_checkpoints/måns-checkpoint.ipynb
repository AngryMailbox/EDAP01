{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "retired-darkness",
   "metadata": {},
   "source": [
    "# Classification with the perceptron and logistic regression\n",
    "\n",
    "__Individual assignment__\n",
    "\n",
    "Author of the assignment: Pierre Nugues\n",
    "\n",
    "__Student name__: Måns Alklint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "former-expense",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "The objectives of this second assignment are to:\n",
    "\n",
    "1.  Write a linear regression program using gradient descent;\n",
    "2.  Write linear classifiers using the perceptron algorithm and logistic regression;\n",
    "3.  Experiment variations of the algorithms;\n",
    "4.  Evaluate your classifiers;\n",
    "5.  Experiment with popular tools;\n",
    "6.  Read a scientific article on optimization techniques and comment it;\n",
    "7.  Present your code, results, and comments in a short dissertation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lasting-booking",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "The gradient descent is a basic technique to estimate the parameters of cost functions. \n",
    "1. You will first use the gradient descent method to implement linear regression. \n",
    "2. You will then program the perceptron algorithm. \n",
    "3. Finally, you will improve the threshold function with the logistic curve (logistic regression). \n",
    "\n",
    "You will try various configurations and study their influence on the learning speed and accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accredited-salad",
   "metadata": {},
   "source": [
    "##  Programming language\n",
    "As programming language, you will use Python and write your code in this notebook.\n",
    "\n",
    "You need to have a comprehensive Python distribution such as Anaconda (https://www.anaconda.com/products/individual). This distribution is available on the student computers at the computer science department.\n",
    "Finally, you start a notebook by typing:\n",
    "\n",
    "`jupyter lab`\n",
    "\n",
    "in a terminal window and you select the notebook by clicking on it in the left pane.\n",
    "You run the pieces of code by typing shift+enter. You can also use a programming tool like VS Code or PyCharm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aerial-bedroom",
   "metadata": {},
   "source": [
    "## Imports\n",
    "Imports you may use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "complete-mileage",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "similar-stanford",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "You will implement the gradient descent method as explained in pages 694-697 in Russell-Norvig (pp. 719--720 in the 3rd ed.) and in the slides to compute regression lines. You will implement the stochastic and batch versions of the algorithm.\n",
    "\n",
    "You must try to do it yourself first. If you encounter difficulties, you also have the solution to this exercise in the section _Solution to linear regression_ below. See: https://github.com/pnugues/edap01/tree/master/gradient_descent_practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "included-privacy",
   "metadata": {},
   "source": [
    "### Your implementation of linear regression\n",
    "You will implement a regression program to predict the counts of _A_'s in a text from the total count of letters. You will apply it on two data sets corresponding to letter counts in the 15 chapters of the French and English versions of _Salammbô_, where the first column is the total count of characters and the second one, the count of A's. \n",
    "\n",
    "Start with either French or English and when your program ready, test it on the other language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "southern-representative",
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_fr = np.array([[36961, 2503],\n",
    "                    [43621, 2992],\n",
    "                    [15694, 1042],\n",
    "                    [36231, 2487],\n",
    "                    [29945, 2014],\n",
    "                    [40588, 2805],\n",
    "                    [75255, 5062],\n",
    "                    [37709, 2643],\n",
    "                    [30899, 2126],\n",
    "                    [25486, 1784],\n",
    "                    [37497, 2641],\n",
    "                    [40398, 2766],\n",
    "                    [74105, 5047],\n",
    "                    [76725, 5312],\n",
    "                    [18317, 1215]])\n",
    "\n",
    "stat_en = np.array([[35680, 2217],\n",
    "                    [42514, 2761],\n",
    "                    [15162, 990],\n",
    "                    [35298, 2274],\n",
    "                    [29800, 1865],\n",
    "                    [40255, 2606],\n",
    "                    [74532, 4805],\n",
    "                    [37464, 2396],\n",
    "                    [31030, 1993],\n",
    "                    [24843, 1627],\n",
    "                    [36172, 2375],\n",
    "                    [39552, 2560],\n",
    "                    [72545, 4597],\n",
    "                    [75352, 4871],\n",
    "                    [18031, 1119]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuffed-albania",
   "metadata": {},
   "source": [
    "From the datasets above, tell what is ${X}$ and $\\mathbf{y}$. Extract:\n",
    "1. The ${X}$ matrix, where you will have a column to model the intercept;\n",
    "2. The $\\mathbf{y}$ vector\n",
    "\n",
    "from these arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "ultimate-remark",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x-axis: [[1.0000e+00 3.5680e+04]\n",
      " [1.0000e+00 4.2514e+04]\n",
      " [1.0000e+00 1.5162e+04]\n",
      " [1.0000e+00 3.5298e+04]\n",
      " [1.0000e+00 2.9800e+04]\n",
      " [1.0000e+00 4.0255e+04]\n",
      " [1.0000e+00 7.4532e+04]\n",
      " [1.0000e+00 3.7464e+04]\n",
      " [1.0000e+00 3.1030e+04]\n",
      " [1.0000e+00 2.4843e+04]\n",
      " [1.0000e+00 3.6172e+04]\n",
      " [1.0000e+00 3.9552e+04]\n",
      " [1.0000e+00 7.2545e+04]\n",
      " [1.0000e+00 7.5352e+04]\n",
      " [1.0000e+00 1.8031e+04]] \n",
      "\n",
      "y-axis: [2217 2761  990 2274 1865 2606 4805 2396 1993 1627 2375 2560 4597 4871\n",
      " 1119] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import regression as reg\n",
    "import numpy as np\n",
    "\n",
    "stat_en = np.array([[35680, 2217],\n",
    "                    [42514, 2761],\n",
    "                    [15162, 990],\n",
    "                    [35298, 2274],\n",
    "                    [29800, 1865],\n",
    "                    [40255, 2606],\n",
    "                    [74532, 4805],\n",
    "                    [37464, 2396],\n",
    "                    [31030, 1993],\n",
    "                    [24843, 1627],\n",
    "                    [36172, 2375],\n",
    "                    [39552, 2560],\n",
    "                    [72545, 4597],\n",
    "                    [75352, 4871],\n",
    "                    [18031, 1119]])\n",
    "\n",
    "val = np.c_[np.ones(stat_en.shape[0]), stat_en[:, 0]]\n",
    "y = stat_en[:, 1]\n",
    "\n",
    "print(\"x-axis:\",val,\"\\n\")\n",
    "print(\"y-axis:\",y,\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disabled-facing",
   "metadata": {},
   "source": [
    "Scale the arrays so that they fit in the range [0, 1] on the $x$ and $y$ axes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "confidential-purple",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x-axis: [[1.         0.47351099]\n",
      " [1.         0.56420533]\n",
      " [1.         0.20121563]\n",
      " [1.         0.46844145]\n",
      " [1.         0.39547723]\n",
      " [1.         0.53422603]\n",
      " [1.         0.98911774]\n",
      " [1.         0.49718654]\n",
      " [1.         0.41180062]\n",
      " [1.         0.32969264]\n",
      " [1.         0.48004034]\n",
      " [1.         0.52489649]\n",
      " [1.         0.96274817]\n",
      " [1.         1.        ]\n",
      " [1.         0.23929026]] \n",
      "\n",
      "y-axis: [[0.45514268]\n",
      " [0.56682406]\n",
      " [0.20324369]\n",
      " [0.46684459]\n",
      " [0.38287826]\n",
      " [0.53500308]\n",
      " [0.98645042]\n",
      " [0.49189078]\n",
      " [0.40915623]\n",
      " [0.33401766]\n",
      " [0.48757955]\n",
      " [0.52555943]\n",
      " [0.94374872]\n",
      " [1.        ]\n",
      " [0.22972696]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import regression as reg\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#y= y.reshape(-1,1)\n",
    "y = np.array([y]).T\n",
    "val, maxX= reg.normalize(val)\n",
    "y, maxY= reg.normalize(y)\n",
    "\n",
    "print(\"x-axis:\",val,\"\\n\")\n",
    "print(\"y-axis:\",y,\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "herbal-valve",
   "metadata": {},
   "source": [
    "#### Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84842b1",
   "metadata": {},
   "source": [
    "Some goop to make it work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d8458b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sse(X, y, w):\n",
    "    \"\"\"\n",
    "    Calculates the sum of squared errors. (den variation y har kring y-hatt (regressionslinjen). Alltså den variation som återstår\n",
    "    och ej förklaras av det linjära sambandet mellan y och x)\n",
    "    :param X: The input matrix\n",
    "    :param y: The output vector\n",
    "    :param w: The weights\n",
    "    :return: The sum of squared errors\n",
    "    \"\"\"\n",
    "    error = y - X @ w\n",
    "    return error.T @ error\n",
    "\n",
    "\n",
    "def predict(X, w):\n",
    "    \"\"\"\n",
    "    Predicts the output of a linear model by multiplying the input matrix with the weights\n",
    "    :param X: The input matrix\n",
    "    :param w: The weights\n",
    "    :return: The predicted output\n",
    "    \"\"\"\n",
    "    return X @ w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "timely-anaheim",
   "metadata": {},
   "source": [
    "Implement the descent functions. You will pass `X`, `y`, the learning rate in the $\\alpha$ variable, the initial weight vector in `w`, the tolerance in the $\\epsilon$ variable, the maximal number of epochs in `epochs`. You will return `w`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "challenging-shopper",
   "metadata": {},
   "source": [
    "Batch descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressive-thickness",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "def fit_batch(X, y, alpha, w,\n",
    "              epochs=500,\n",
    "              epsilon=1.0e-5):\n",
    "    alpha /= len(X)\n",
    "    for epoch in range(epochs+1): # +1 to make sure we do at least one iteration if supposed to be inclusive?\n",
    "        error = predict(X, w) - y\n",
    "        gradient = X.T @ error\n",
    "        w = w - alpha * gradient\n",
    "        if np.linalg.norm(gradient) < epsilon:\n",
    "            break\n",
    "    print(\"Epoch\", epoch)\n",
    "    return w\n",
    "\n",
    "#print(\"Weights:\", fit_batch(x, y, 0.01, np.zeros(x.shape[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arctic-tobacco",
   "metadata": {},
   "source": [
    "Stochastic descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improved-surrey",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Write your code here\n",
    "def fit_stoch(X, y, alpha, w,\n",
    "              epochs=500,\n",
    "              epsilon=1.0e-5):\n",
    "    global logs, logs_stoch\n",
    "    logs = []\n",
    "    logs_stoch = []\n",
    "    np.random.seed(0)\n",
    "    idx = list(range(len(X)))\n",
    "    for epoch in range(epochs+1):\n",
    "        np.random.shuffle(idx)\n",
    "        for i in idx:\n",
    "            # error is a scalar\n",
    "            error = (predict(X[i], w) - y[i])[0]\n",
    "            gradient = error * X[i:i + 1].T\n",
    "            w = w - alpha * gradient\n",
    "            logs_stoch += (w, alpha, sse(X, y, w))\n",
    "        if np.linalg.norm(gradient) < epsilon:\n",
    "            break\n",
    "        logs += (w, alpha, sse(X, y, w))\n",
    "    print(\"Epoch\", epoch)\n",
    "    return w\n",
    "\n",
    "#print(\"Weights:\", fit_stoch(x, y, 0.01, np.zeros(x.shape[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becoming-upset",
   "metadata": {},
   "source": [
    "#### Applying batch descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atomic-given",
   "metadata": {},
   "source": [
    "Apply the batch descent and print the final weight values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equipped-consensus",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"===Batch descent===\")\n",
    "w = np.zeros((val.shape[1], 1))\n",
    "w = fit_batch(val, y, 1, w)\n",
    "print(\"Weights:\\n\", w)\n",
    "print(\"SSE:\\n\", sse(val, y, w))\n",
    "#if Normalized:\n",
    "maxima = np.concatenate((maxX, maxY))\n",
    "maxima = maxima.reshape(-1, 1)\n",
    "w = maxima[-1, 0] * (w / maxima[:-1, 0:1])\n",
    "print(\"Restored weights:\\n\", w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heavy-asbestos",
   "metadata": {},
   "source": [
    "Visualize the points of your dataset as well as the regression lines you obtain using matplotlib or another similar program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "variable-sphere",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize the points of your dataset as well as the regression lines you obtain using matplotlib or another similar program.\n",
    "\n",
    "def plot(X, y, w):\n",
    "    # Write your code here\n",
    "    x_fig = [X[i][1] * maxX[1] for i in range(len(X))]\n",
    "    y_fig = [yi * maxY for yi in y]\n",
    "    plt.scatter(x_fig, y_fig)\n",
    "    plt.plot([min(x_fig), max(x_fig)],\n",
    "         [[1, min(x_fig)] @ w, [1, max(x_fig)] @ w])\n",
    "    plt.show()\n",
    "\n",
    "#w = fit_stoch(x, y, 0.01, np.zeros(x.shape[1]))\n",
    "plot(val, y, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "higher-relations",
   "metadata": {},
   "source": [
    "#### Do you mean \"[Applying] stochastic descent\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charged-memory",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"===Stochastic descent===\")\n",
    "w = np.zeros((val.shape[1], 1))\n",
    "w = fit_stoch(val, y, 1, w)\n",
    "print(\"Weights\\n\", w)\n",
    "print(\"SSE\\n\", sse(val, y, w))\n",
    "if 2:\n",
    "    maxima = maxima.reshape(-1, 1)\n",
    "    w = maxima[-1, 0] * (w / maxima[:-1, 0:1])\n",
    "    print(\"Restored weights\\n\", w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dependent-concentration",
   "metadata": {},
   "source": [
    "Visualize the points of your dataset as well as the regression lines you obtain using matplotlib or another similar program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specific-least",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(val, y, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sudden-collapse",
   "metadata": {},
   "source": [
    "### A solution to linear regression\n",
    "\n",
    "To help you start this assignment, your instructor wrote two Python notebooks that solve this exercise on linear regression. You can find them here: https://github.com/pnugues/edap01/tree/master/gradient_descent_practice\n",
    "\n",
    "2. The first notebook, `gradient_descent_numpy.ipynb`, uses Numpy. It is more compact, but you need to know a bit of numpy, for instance you multiply matrix `M` by matrix `N` with the operation `M @ N`\n",
    "1. The second notebook, `gradient_descent.ipynb`, only uses Python. The vector and matrix operations such as the dot product that are in the `vector.py` file. You can see how your instructor write the dot product or matrix multiplication operations so that there is no magic as with NumPy\n",
    "\n",
    "\n",
    "To run these programs, download them on your computer as well as the other program in the import list: vector.py\n",
    "\n",
    "The programs are also available as Python programs from\n",
    "https://github.com/pnugues/ilppp/tree/master/programs/ch04/python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acoustic-register",
   "metadata": {},
   "source": [
    "## Classification\n",
    "You will use the same data set as for linear regression, but this time to classify a chapter as French or English. Given a pair of numbers corresponding the letter count and count of _A_, you will predict the language:\n",
    "1. $\\mathbf{x} = (35680, 2217)$ $\\to$ $y$ = English\n",
    "2. $\\mathbf{x} = (37497, 2641)$ $\\to$ $y$ = French"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surrounded-belgium",
   "metadata": {},
   "source": [
    "### The dataset\n",
    "You will use the arrays below:\n",
    "1. `X` contains the counts of letters and of _A_ s as well as a column of ones for the intercept;\n",
    "2. `y` contains the classes, where 0 is for English and 1 for French."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acceptable-coordination",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1.0, 35680.0, 2217.0],\n",
    "              [1.0, 42514.0, 2761.0],\n",
    "              [1.0, 15162.0, 990.0],\n",
    "              [1.0, 35298.0, 2274.0],\n",
    "              [1.0, 29800.0, 1865.0],\n",
    "              [1.0, 40255.0, 2606.0],\n",
    "              [1.0, 74532.0, 4805.0],\n",
    "              [1.0, 37464.0, 2396.0],\n",
    "              [1.0, 31030.0, 1993.0],\n",
    "              [1.0, 24843.0, 1627.0],\n",
    "              [1.0, 36172.0, 2375.0],\n",
    "              [1.0, 39552.0, 2560.0],\n",
    "              [1.0, 72545.0, 4597.0],\n",
    "              [1.0, 75352.0, 4871.0],\n",
    "              [1.0, 18031.0, 1119.0],\n",
    "              [1.0, 36961.0, 2503.0],\n",
    "              [1.0, 43621.0, 2992.0],\n",
    "              [1.0, 15694.0, 1042.0],\n",
    "              [1.0, 36231.0, 2487.0],\n",
    "              [1.0, 29945.0, 2014.0],\n",
    "              [1.0, 40588.0, 2805.0],\n",
    "              [1.0, 75255.0, 5062.0],\n",
    "              [1.0, 37709.0, 2643.0],\n",
    "              [1.0, 30899.0, 2126.0],\n",
    "              [1.0, 25486.0, 1784.0],\n",
    "              [1.0, 37497.0, 2641.0],\n",
    "              [1.0, 40398.0, 2766.0],\n",
    "              [1.0, 74105.0, 5047.0],\n",
    "              [1.0, 76725.0, 5312.0],\n",
    "              [1.0, 18317.0, 1215.0]])\n",
    "y = np.array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
    "              1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collected-package",
   "metadata": {},
   "source": [
    "We visualize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sudden-making",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_fr = [x[1] for i, x in enumerate(X) if y[i] == 1]\n",
    "y_fr = [x[2] for i, x in enumerate(X) if y[i] == 1]\n",
    "val = [x[1] for i, x in enumerate(X) if y[i] == 0]\n",
    "y = [x[2] for i, x in enumerate(X) if y[i] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selective-retrieval",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x_fr, y_fr, color='red')\n",
    "plt.scatter(val, y, color='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classified-chair",
   "metadata": {},
   "source": [
    "### Normalize the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norman-engagement",
   "metadata": {},
   "source": [
    "Gradient descent algorithms can be very sensitive to the range. Therefore, we normalize the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "abroad-clothing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(Xy):\n",
    "    maxima = np.amax(Xy, axis=0)\n",
    "    Xy = 1/maxima * Xy\n",
    "    return (Xy, maxima)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "intellectual-arcade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.46503747, 0.41735693],\n",
       "       [1.        , 0.55410883, 0.51976657],\n",
       "       [1.        , 0.19761486, 0.18637048],\n",
       "       [1.        , 0.46005865, 0.42808735],\n",
       "       [1.        , 0.38840013, 0.35109187],\n",
       "       [1.        , 0.52466601, 0.49058735],\n",
       "       [1.        , 0.9714174 , 0.90455572],\n",
       "       [1.        , 0.48828935, 0.45105422],\n",
       "       [1.        , 0.40443141, 0.37518825],\n",
       "       [1.        , 0.32379277, 0.30628765],\n",
       "       [1.        , 0.47144998, 0.4471009 ],\n",
       "       [1.        , 0.51550342, 0.48192771],\n",
       "       [1.        , 0.94551971, 0.8653991 ],\n",
       "       [1.        , 0.98210492, 0.91698042],\n",
       "       [1.        , 0.23500815, 0.21065512],\n",
       "       [1.        , 0.48173346, 0.47119729],\n",
       "       [1.        , 0.56853698, 0.56325301],\n",
       "       [1.        , 0.20454871, 0.19615964],\n",
       "       [1.        , 0.47221896, 0.46818524],\n",
       "       [1.        , 0.39029   , 0.37914157],\n",
       "       [1.        , 0.52900619, 0.5280497 ],\n",
       "       [1.        , 0.98084066, 0.95293675],\n",
       "       [1.        , 0.49148257, 0.49755271],\n",
       "       [1.        , 0.40272401, 0.4002259 ],\n",
       "       [1.        , 0.33217335, 0.33584337],\n",
       "       [1.        , 0.48871945, 0.4971762 ],\n",
       "       [1.        , 0.52652981, 0.52070783],\n",
       "       [1.        , 0.96585207, 0.95011295],\n",
       "       [1.        , 1.        , 1.        ],\n",
       "       [1.        , 0.23873574, 0.22872741]])"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_norm, maxima = normalize(X)\n",
    "X_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ancient-stewart",
   "metadata": {
    "tags": []
   },
   "source": [
    "## The Perceptron\n",
    "\n",
    "1. Write the perceptron program as explained in pages 700--702 in Russell-Norvig (pp. 723--725 in the 3rd ed.) and in the slides and run it on your data set. As suggested program structure, use two functions: \n",
    " * `fit(X, y)` that will return `w` (the model). You can choose a stochastic or batch variant;\n",
    " * `predict(X, w)` that will return `y_hat`. You can encapsulate these functions in a class and, of course, add more parameters.\n",
    "2. As a stop criterion, you will use the number of misclassified examples.\n",
    "3. You will report the parameters you have used and the weight vector\n",
    "\n",
    "You can use numpy or not. The next cells are just suggested steps. You can implement it your way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worthy-singing",
   "metadata": {},
   "source": [
    "### The `predict(X, w)` function\n",
    "Write a `predict(X, w)` function that given a matrix of observations ${X}$ and a weight vector $\\mathbf{w}$ will return a $\\mathbf{\\hat{y}}$ vector classes (0 or 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "applicable-insight",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, w):\n",
    "    \"\"\"\n",
    "    Predictor function for logistic regression.\n",
    "    \n",
    "    Args:\n",
    "        X (numpy.ndarray): Input data matrix of shape (n_samples, n_features).\n",
    "        w (numpy.ndarray): Weight vector of shape (n_features,).\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: Predicted class labels of shape (n_samples,).\n",
    "    \"\"\"\n",
    "    scores = np.dot(X, w)\n",
    "    #print(\"Scores:\", scores)\n",
    "    return (scores >= 0).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elementary-smoke",
   "metadata": {},
   "source": [
    "### The `fit(X, y)` function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interior-voice",
   "metadata": {},
   "source": [
    "Write a `fit(X, y)` function that given a matrix of observations ${X}$ and a vector of responses $\\mathbf{y}$ will return a weight $\\mathbf{w}$ vector. You may use the other arguments of the function, notably the number of misclassified examples to define the stop condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "provincial-stand",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def fit_stoch2(X, y, epochs=1000, learning_rate=0.01, max_misclassified=0, verbose=True):\n",
    "    n_samples, n_features = X.shape\n",
    "    w = np.zeros(n_features)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        misclassified = 0\n",
    "        for i in range(n_samples):\n",
    "            xi = X[i]\n",
    "            yi = y[i]\n",
    "            y_pred = predict(xi.reshape(1, -1), w)  # Use predict function\n",
    "            \n",
    "            if y_pred != yi:\n",
    "                misclassified += 1\n",
    "                w -= learning_rate * (y_pred - yi) * xi\n",
    "            \n",
    "            if misclassified >= max_misclassified:\n",
    "                break\n",
    "        \n",
    "        if verbose and epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}: Misclassified {misclassified}/{n_samples}\")\n",
    "        \n",
    "        if misclassified <= max_misclassified:\n",
    "            break\n",
    "    \n",
    "    return w\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frozen-demand",
   "metadata": {},
   "source": [
    "### Fitting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "chinese-serial",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Misclassified 1/30\n",
      "Epoch 100: Misclassified 1/30\n",
      "Epoch 200: Misclassified 1/30\n",
      "Epoch 300: Misclassified 1/30\n",
      "Epoch 400: Misclassified 1/30\n",
      "Epoch 500: Misclassified 1/30\n",
      "Epoch 600: Misclassified 1/30\n",
      "Epoch 700: Misclassified 1/30\n",
      "Epoch 800: Misclassified 1/30\n",
      "Epoch 900: Misclassified 1/30\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.00142681, 0.00066352, 0.00059549])"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = fit_stoch2(X_norm, y)\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "racial-antarctica",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restored weights [7.5792239786495434, 4.593839237382015e-05, 0.0005954897654672411]\n",
      "Weights with y set to 1 [12727.714930083865, 0.07714388229288098, 1.0]\n"
     ]
    }
   ],
   "source": [
    "w = [w[i] * maxima[-1] / maxima[i] for i in range(len(w))]\n",
    "print(\"Restored weights\", w)\n",
    "w = [w[j] / w[-1] for j in range(len(w))]\n",
    "print(\"Weights with y set to 1\", w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "editorial-combination",
   "metadata": {},
   "source": [
    "### Visualizing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "alone-granny",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must be the same size",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[222], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m plt\u001b[38;5;241m.\u001b[39mscatter(x_fr, y_fr, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mblue\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot([\u001b[38;5;28mmin\u001b[39m(x_fr \u001b[38;5;241m+\u001b[39m val), \u001b[38;5;28mmax\u001b[39m(x_fr \u001b[38;5;241m+\u001b[39m val)],\n\u001b[0;32m      4\u001b[0m          [\u001b[38;5;241m-\u001b[39mw[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mmin\u001b[39m(x_fr \u001b[38;5;241m+\u001b[39m val) \u001b[38;5;241m-\u001b[39m w[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39mw[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mmax\u001b[39m(x_fr \u001b[38;5;241m+\u001b[39m val) \u001b[38;5;241m-\u001b[39m w[\u001b[38;5;241m0\u001b[39m]])\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\matplotlib\\pyplot.py:3699\u001b[0m, in \u001b[0;36mscatter\u001b[1;34m(x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, edgecolors, plotnonfinite, data, **kwargs)\u001b[0m\n\u001b[0;32m   3680\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mscatter)\n\u001b[0;32m   3681\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscatter\u001b[39m(\n\u001b[0;32m   3682\u001b[0m     x: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m ArrayLike,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3697\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3698\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m PathCollection:\n\u001b[1;32m-> 3699\u001b[0m     __ret \u001b[38;5;241m=\u001b[39m \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscatter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3701\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3702\u001b[0m \u001b[43m        \u001b[49m\u001b[43ms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3703\u001b[0m \u001b[43m        \u001b[49m\u001b[43mc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3704\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmarker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmarker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3705\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcmap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcmap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3706\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3707\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvmin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3708\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvmax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvmax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3709\u001b[0m \u001b[43m        \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3710\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlinewidths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlinewidths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3711\u001b[0m \u001b[43m        \u001b[49m\u001b[43medgecolors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medgecolors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3712\u001b[0m \u001b[43m        \u001b[49m\u001b[43mplotnonfinite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mplotnonfinite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3713\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3714\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3715\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3716\u001b[0m     sci(__ret)\n\u001b[0;32m   3717\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m __ret\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\matplotlib\\__init__.py:1478\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[1;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1475\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m   1476\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(ax, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1477\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1478\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43max\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msanitize_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1480\u001b[0m     bound \u001b[38;5;241m=\u001b[39m new_sig\u001b[38;5;241m.\u001b[39mbind(ax, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1481\u001b[0m     auto_label \u001b[38;5;241m=\u001b[39m (bound\u001b[38;5;241m.\u001b[39marguments\u001b[38;5;241m.\u001b[39mget(label_namer)\n\u001b[0;32m   1482\u001b[0m                   \u001b[38;5;129;01mor\u001b[39;00m bound\u001b[38;5;241m.\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(label_namer))\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\matplotlib\\axes\\_axes.py:4655\u001b[0m, in \u001b[0;36mAxes.scatter\u001b[1;34m(self, x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, edgecolors, plotnonfinite, **kwargs)\u001b[0m\n\u001b[0;32m   4653\u001b[0m y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mma\u001b[38;5;241m.\u001b[39mravel(y)\n\u001b[0;32m   4654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m!=\u001b[39m y\u001b[38;5;241m.\u001b[39msize:\n\u001b[1;32m-> 4655\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y must be the same size\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   4657\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m s \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   4658\u001b[0m     s \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m20\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mpl\u001b[38;5;241m.\u001b[39mrcParams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_internal.classic_mode\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m\n\u001b[0;32m   4659\u001b[0m          mpl\u001b[38;5;241m.\u001b[39mrcParams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlines.markersize\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2.0\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: x and y must be the same size"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(x_fr, y_fr, color='red')\n",
    "print(len(val))\n",
    "print(len(y))\n",
    "plt.scatter(val, y, color='blue')\n",
    "plt.plot([min(x_fr + val), max(x_fr + val)],\n",
    "         [-w[1] * min(x_fr + val) - w[0], -w[1] * max(x_fr + val) - w[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grateful-liberia",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "Evaluate your perceptron using the leave-one-out cross validation method. You will have to train and run 30 models. In each train/run session, you will train on 29 samples and evaluate on the remaining sample. You have then either a correct or a wrong classification. You will sum these classifications, i.e. the number of correct classifications, to get your final evaluation, for instance 29/30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tight-pilot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "def leave_one_out_cross_val(X, y, fitting_function):\n",
    "    score = 0\n",
    "    ...\n",
    "    return score / len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transsexual-malpractice",
   "metadata": {},
   "outputs": [],
   "source": [
    "stoch_accuracy = leave_one_out_cross_val(X_norm, y, fit_stoch)\n",
    "print('Cross-validation accuracy (stochastic):', stoch_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abandoned-saint",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "From your perceptron program, implement logistic regression. You can either follow the description from the slides or from the textbook, S. Russell and R. Norvig, _Artificial Intelligence_, 2022, pages 702--704 (pp. 725--727 in the 3rd ed.). Note that the textbook uses a criterion that is rejected by most practioneers. You can either implement the stochastic or the batch version of the algorithm, or both versions. As stop criterion, you will use either the norm of the gradient or the norm of the difference between two consecutive weight vectors. You will also set a maximal number of epochs. Run the resulting program on your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "treated-affiliation",
   "metadata": {},
   "source": [
    "Write the logistic function, where the $\\mathbf{x}$ input is a vector of real numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composite-broadcast",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "def logistic(x):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funded-violation",
   "metadata": {},
   "source": [
    "### The `predict(X, w)` function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alleged-delaware",
   "metadata": {},
   "source": [
    "Write a `predict_proba()` function that given a matrix of observations ${X}$ and a weight vector $\\mathbf{w}$ will return a vector of probabilities to belong to class 1: The vector will consist of $P(1|\\mathbf{x}_i)$ for all the $i$ rows of ${X}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parental-winning",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "def predict_proba(X, w):\n",
    "    return X @ w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "olive-equilibrium",
   "metadata": {},
   "source": [
    "Write a `predict(X, w)` function that given a matrix of observations ${X}$ and a weight vector $\\mathbf{w}$ will return the class. You will use `predict_proba()` and set the threshold to belong to class 1 to 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "directed-freeze",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, w):\n",
    "    # Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weird-replication",
   "metadata": {},
   "source": [
    "### The `fit(X, y)` function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electric-bunny",
   "metadata": {},
   "source": [
    "You will now write the `fit(X, y)` function as with the perceptron. You may call it `fit_stoch(X, y)` or `fit_batch(X, y)`. Use the parameters given in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forward-concentrate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "import random\n",
    "\n",
    "\n",
    "def fit_stoch(X, y, alpha=100,\n",
    "              epochs=1000,\n",
    "              epsilon=1.0e-4,\n",
    "              verbose=False):\n",
    "    w = np.zeros(X.shape[1])\n",
    "    ...\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "presidential-adoption",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = fit_stoch(X_norm, y, verbose=True)\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executed-history",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = [w[i] / maxima[i] for i in range(len(w))]\n",
    "print(\"Restored weights\", w)\n",
    "w = [w[j] / w[-1] for j in range(len(w))]\n",
    "print(\"Weights with y set to 1\", w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contemporary-correlation",
   "metadata": {},
   "source": [
    "### Visualizing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "headed-award",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x_fr, y_fr, color='red')\n",
    "plt.scatter(val, y, color='blue')\n",
    "plt.plot([min(x_fr + val), max(x_fr + val)],\n",
    "         [-w[1] * min(x_fr + val) - w[0], -w[1] * max(x_fr + val) - w[0]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exclusive-database",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "Evaluate your logistic regression using the leave-one-out cross validation method as with the perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indoor-truck",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "def leave_one_out_cross_val(X, y, fitting_function):\n",
    "    score = 0\n",
    "    ...\n",
    "    return score / len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strong-alaska",
   "metadata": {},
   "outputs": [],
   "source": [
    "stoch_accuracy = leave_one_out_cross_val(X, y, fit_stoch)\n",
    "print('Cross-validation accuracy (batch):', stoch_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incident-future",
   "metadata": {},
   "source": [
    "## Visualizing the logistic surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attached-occasion",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_logistic_surf(x_range, y_range, w_opt):\n",
    "    x_axis, y_axis = np.meshgrid(x_range, y_range)\n",
    "\n",
    "    # We compute the probability surface as a function of x and y\n",
    "    # First generate the (1, x, y) tuples\n",
    "    grid = np.array([[[1.0, i, j] for j in y_range] for i in x_range])\n",
    "    # Then, compute logistic((1, x, y) . w_opt)\n",
    "    z_axis = logistic((grid @ w_opt))\n",
    "    return x_axis, y_axis, z_axis.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "harmful-identity",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_range = np.linspace(0, 100000, 200)\n",
    "y_range = np.linspace(0, 10000, 200)\n",
    "# w = [2.073225839414742, -0.049125455233437906, 0.7440143556104162]\n",
    "\n",
    "x_axis, y_axis, z_axis = plot_logistic_surf(x_range, y_range, w)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = Axes3D(fig, auto_add_to_figure=False)\n",
    "fig.add_axes(ax)\n",
    "\n",
    "surf = ax.plot_surface(y_axis, x_axis, z_axis, rstride=1, cstride=1, cmap=cm.coolwarm,\n",
    "                       linewidth=0, antialiased=False, alpha=0.2)\n",
    "fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "# We plot the observations\n",
    "for val, y_class in zip(X, y):\n",
    "    if y_class == 1:\n",
    "        ax.scatter(val[2], val[1], y_class, color='green', marker='x')\n",
    "    else:\n",
    "        ax.scatter(val[2], val[1], y_class, color='red', marker='x')\n",
    "\n",
    "ax.elev = 30\n",
    "ax.azim = -150\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "internal-harassment",
   "metadata": {},
   "source": [
    "## Programming logistic regression with popular APIs\n",
    "Should you use logistic regression in a project, you will probably resort to existing libraries. In the next cells, you will apply the logistic regression classification with two popular APIs:\n",
    "1. sklearn\n",
    "2. Keras\n",
    "\n",
    "`sklearn` is included in anaconda.\n",
    "You will install the rest with:\n",
    "```\n",
    "pip install --upgrade keras-core keras-nightly \n",
    "```\n",
    "You will read and run the code in the three separate notebooks."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "promising-spring",
   "metadata": {},
   "source": [
    "## Reading\n",
    "You will read the article *An overview of gradient descent optimization algorithms* by Ruder (2017) and you will outline the main characteristics of all the optimization algorithms the author describes. This part should be of about one to two pages. Link to the article: https://arxiv.org/abs/1609.04747.\n",
    "\n",
    "You can also visualize the descents of the algorithm variants on Ruder's webpage: https://www.ruder.io/optimizing-gradient-descent/.\n",
    "\n",
    "If you understand French, or using Google translate, you may also want to read the original article on gradient descent by Cauchy here:  https://gallica.bnf.fr/ark:/12148/bpt6k2982c/f540.item.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "level-lodging",
   "metadata": {},
   "source": [
    "### Report\n",
    "\n",
    "The assignment must be documented in the report, which should contain the following:\n",
    "\n",
    "*   The name of the author, the title of the assignment, and any relevant information on the front page;\n",
    "*   A presentation of the assignment and the possible improvements you would have brought;\n",
    "*   A presentation of your implementation;\n",
    "*   A print-out of the example set(s) and the resulting weight vectors;\n",
    "*   Comments on the results you have obtained, including your cross validation;\n",
    "*   A short dissertation on the optimization algorithms from Ruder's paper.\n",
    "\n",
    "Please, typeset and format your report consistently. You must use Latex. Documents written using MS Word or any similar format will not be considered.\n",
    "\n",
    "You may have a look at the code in the textbook code repository (or any other implementations), but the code you hand in must be your work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "damaged-blackberry",
   "metadata": {},
   "source": [
    "## Submission\n",
    "Submit the notebook and the report to Canvas (two files). Do not include the code printout in the report, but only comments on its interesting parts. You will submit the notebook as a separate file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjacent-guarantee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "b97b11a820675205aae8f1d7f2a3f22bbd3a2c30189f44042310baf5b4cd1987"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
